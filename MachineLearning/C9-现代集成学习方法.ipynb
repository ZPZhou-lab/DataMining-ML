{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "debd8b04-c979-43f8-9df4-2601034d997d",
   "metadata": {},
   "source": [
    "# **Chap9：现代集成学习方法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "873c0c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import resample\n",
    "from copy import deepcopy\n",
    "import seaborn as sns\n",
    "colors = sns.palettes.color_palette(\"muted\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd430ae7",
   "metadata": {},
   "source": [
    "本章介绍三种**集成学习**（**ensemble model**）方法，包括：\n",
    "* 梯度提升变种：**XGBoost**\n",
    "* 梯度提升变种：**LightGBM**\n",
    "* **Stacking**  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25bb7e5f",
   "metadata": {},
   "source": [
    "它们是目前人们使用最多，经过实践检验性能最优的方法，在处理大部分机器学习问题时，人们都会首先考虑尝试这几种方法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d93b4c72",
   "metadata": {},
   "source": [
    "由于**XGBoost和LightGBM更多是工程上对梯度提升算法的进一步改造和优化**，本质上与梯度提升方法没有太大区别，我们**仅介绍模型的改进思想**，**具体的实现是工程问题而非机器学习算法问题**，更重要的是学会正确使用这些算法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "464780d9",
   "metadata": {},
   "source": [
    "## **1 XBGoost**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f2a0302",
   "metadata": {},
   "source": [
    "### **1.1 梯度提升方法总结**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98356294",
   "metadata": {},
   "source": [
    "在进一步介绍XGBoost之前，我们先对**梯度提升算法**（**Gradient Boosting**）的优缺点做简单总结和对比"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31bc9b0e",
   "metadata": {},
   "source": [
    "**(1) 优点**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b8db2a0",
   "metadata": {},
   "source": [
    "* 采用**梯度下降**思想，前向分步算法的每次迭代拟合当前损失函数的”残差“（**负梯度**），能够处理更为一般损失函数下的决策问题\n",
    "    * 例如软件包中经常提供**中位数/绝对值损失**，**Huber损失**，**交叉熵损失**等\n",
    "* 采用**稳健损失**（例如中位数损失替换平方损失）后的梯度提升模型，**对异常分布数据的鲁棒性非常强**\n",
    "* Boosting算法给出的加法模型**不需要基学习器有很强的拟合能力**，**整体模型就能拥有很强的非线性学习能力**\n",
    "* 通常情况下，**当维度不太高**，即 $N > p$ 时，梯度提升方法拥**有很高的预测精度**\n",
    "* 可以**通过Shrinkage和随机采样**对模型进行**正则化**，减少**过拟合**：\n",
    "    * Shrinkage引入一个收缩系数 $\\eta < 1$ 来减小每一个基学习器在最终预测中的贡献，让更多的基学习器有话语权\n",
    "    * 随机采样在每个基学习器训练时，对样本进行随机采样，同时对参与训练的特征进行采样，减少基学习器之间的相关性"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee3a2d0e",
   "metadata": {},
   "source": [
    "**(2) 不足**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d64338df",
   "metadata": {},
   "source": [
    "* 梯度提升**只利用了损失函数的一阶导数信息**，没有利用损失函数的**更高阶导数信息**，而更高阶导数虽然对目标函数提出了更高的可导性要求，但在函数最优化时往往更有效\n",
    "* 梯度提升方法**没有讨论任何基学习器模型复杂度的问题**，模型正则化的调节需要人们凭借经验”手动“完成\n",
    "* **软件包中只支持CART树作为基学习器**，不支持其他学习器（例如逻辑回归）\n",
    "* **软件包只支持有限的损失函数**，即编写软件包时所提供的几种，在使用时**不支持自定义损失函数**\n",
    "* **高纬数据**（$p$ 很大）时，会带来**巨大的计算问题**，是的模型的训练缓慢甚至不可行 \n",
    "* 基学习器CART树在结点分裂时采用**遍历法**（遍历所有特征、遍历切分点），这一过程**非常耗时**\n",
    "* 作为Boosting算法，当前算法包没有实现**并行化**加速"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "919b95b6",
   "metadata": {},
   "source": [
    "### **1.2 XGBoost模型思想**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bb35436",
   "metadata": {},
   "source": [
    "XGBoost即**Extreme Gradient Boosting**算法的简称，简单来说，它是**基于梯度提升模型**，针对以上各种不足所提出一种**工程化改进**，改进后的算法具有了**分布式特性**，这让XGBoost也成为了工业内首选的机器学习模型之一"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1709bad",
   "metadata": {},
   "source": [
    "#### **1.2.1 带有正则化的损失函数**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fdd21723",
   "metadata": {},
   "source": [
    "回顾梯度提升方法的损失函数及优化目标："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7845e128",
   "metadata": {},
   "source": [
    "$$\n",
    "\\{\\alpha_b^*,\\gamma_b^*\\}_{b=1}^{B} = \\mathop{\\arg\\min}\\limits_{\\alpha_b\\in\\mathbb{R},\\gamma_b\\in\\Theta} \\sum_{i=1}^{N} L\\left( y_i,\\sum_{b=1}^{B}\\alpha_b G_b(x;\\gamma_b) \\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3a12aa7",
   "metadata": {},
   "source": [
    "其中，$L(y,f(x))$ 是模型决策所使用的损失函数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb722097",
   "metadata": {},
   "source": [
    "从**偏差方差分析**的角度：\n",
    "$$\n",
    "\\text{err}(x) = bias^2(x) + var(x) + \\varepsilon^2\n",
    "$$\n",
    "上述优化目标**只能体现模型对数据拟合能力的不断增强，即减小偏差**，**没有体现对模型复杂度的控制**，即**减小方差**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc4a7ade",
   "metadata": {},
   "source": [
    "回顾**Ridge**和**Lasso**，模型的损失函数被写为："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43f33a10",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\text{Ridge：} & \\displaystyle{ \\min \\left( \\frac{1}{N}\\sum_{i=1}^{N}(y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|_2^2\\right) }\\\\\n",
    "\\text{Lasso：} & \\displaystyle{ \\min \\left(\\frac{1}{N}\\sum_{i=1}^{N}(y_i - X_i\\beta)^2 + \\lambda \\|\\beta\\|_1\\right) }\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "194e5f4a",
   "metadata": {},
   "source": [
    "其中，$\\lambda \\|\\beta\\|_2^2$ 和 $\\lambda \\|\\beta\\|_1$ 就是模型对回归系数的**惩罚项**，**起到正则化的作用**，因此也称为**正则化项**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4ceac9",
   "metadata": {},
   "source": [
    "因此，对于一般的机器学习模型，目标函数通常写为**损失函数+正则化项**的形式：\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = L(\\theta) + \\Omega(\\theta)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee601b61",
   "metadata": {},
   "source": [
    "* 损失函数表示模型**拟合数据的情况**，即控制**偏差**\n",
    "* 正则项惩罚复杂的模型，鼓励简单的模型，即控制**方差**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2547b02c",
   "metadata": {},
   "source": [
    "下面的说明**以回归问题为例**，我们仍然**以CART树作为基学习器**，最终的加法模型表示为："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94ed2a6e",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = \\sum_{b=1}^{B}G_b(x;\\gamma_b) = \\sum_{b=1}^{B}{\\left(  \\sum_{j=1}^{J_b} c_{b,j} \\mathbb{I}(x\\in R_{b,j}) \\right)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d920755f",
   "metadata": {},
   "source": [
    "其中，$R_{b,j}$ 表示第 $b$ 棵树的第 $j$ 个叶子区域，$c_{b,j}$ 是对应的该叶子区域的预测值"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "966b8798",
   "metadata": {},
   "source": [
    "XGBoost在损失函数基础上考虑了**叶结点个数**和**每个叶子的输出值**的正则化："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a0c2f2",
   "metadata": {},
   "source": [
    "$$\n",
    "\\Omega(G_b) = \\tau\\cdot J_b + \\frac{1}{2}\\lambda \\|c_b\\|_2^2 = \\tau\\cdot J_b + \\frac{1}{2}\\lambda \\sum_{j=1}^{J_b} c_{b,j}^2\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb439628",
   "metadata": {},
   "source": [
    "其中：\n",
    "* $J_b$ 代表**叶结点个数**，它是对**决策树模型整体复杂度的惩罚**，叶结点越多，模型越复杂，就越容易过拟合\n",
    "* $\\lambda \\|c_b\\|_2^2$ 基于**Ridge**思想，目的是**让叶结点的输出值** $c_{b,j}$ **尽可能平滑**，从而避免过度拟合\n",
    "* $\\tau > 0,\\lambda > 0$ 是正则化超参数，用于控制惩罚的权重"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e342ab55",
   "metadata": {},
   "source": [
    "因此，XGBoost对于决策函数 $f$：\n",
    "$$\n",
    "f(x) = \\sum_{b=1}^{B}G_b(x;\\gamma_b)\n",
    "$$\n",
    "的总体的目标函数可以写作："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f8f2ad9",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L}(f) = \\sum_{i=1}^{N} L(y_i,f(x_i)) + \\sum_{b=1}^{B}\\Omega(G_b) = \\sum_{i=1}^{N} L\\left( y_i,\\sum_{b=1}^{B}G_b(x;\\gamma_b) \\right) + \\sum_{b=1}^{B}\\Omega(G_b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aeb22fc",
   "metadata": {},
   "source": [
    "这样，**XGBoost模型直接从目标函数上考虑了每个基学习器的模型复杂度，而无需我们手动预先设置基学习器超参数以控制复杂度**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c74fd0ca",
   "metadata": {},
   "source": [
    "**求解XGBoost模型需要极小化** $\\mathcal{L}$，注意到，我们**仍然可以使用前向分步算法求解**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e48c46da",
   "metadata": {},
   "source": [
    "假设在算法迭代的第 $b$ 轮，我们已经获得决策函数 $f_{b-1}(x)$：\n",
    "$$\n",
    "f_{b-1}(x) = \\sum_{k=1}^{b-1}G_k(x;\\gamma_k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "608bdf94",
   "metadata": {},
   "source": [
    "此时，我们求解第 $b$ 个基学习器的优化问题："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf3a7da3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\gamma_b & = & \\displaystyle{ \\mathop{\\arg\\min}\\limits_{\\gamma\\in\\Theta} \\sum_{i=1}^{N} L\\left( y_i,f_{b-1}(x_i) +  G_b(x_i;\\gamma_b) \\right) + \\Omega(G_b) }\\\\\n",
    "& = & \\displaystyle{ \\mathop{\\arg\\min}\\limits_{\\gamma\\in\\Theta} \\sum_{i=1}^{N} L\\left( y_i,f_{b-1}(x_i) +  G_b(x_i;\\gamma_b) \\right) + \\tau J_b + \\frac{1}{2}\\lambda \\|c_b\\|_2^2 }\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58e708e7",
   "metadata": {},
   "source": [
    "可以看到。第 $b$ 个基学习器的构建不影响前 $b-1$ 基学习器，因此**正则化项也只需要考虑** $\\omega(G_b)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc1f9ca7",
   "metadata": {},
   "source": [
    "回顾，在没有正则化项的时候，我们可以采用梯度提升相同的思想，**按照梯度下降的方法，用基学习器拟合负梯度**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8d0eba79",
   "metadata": {},
   "source": [
    "#### **1.2.2 使用二阶梯度求解优化问题**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4445b149",
   "metadata": {},
   "source": [
    "梯度下降采用一种**最快下降速度**的思想，导出**负梯度方向是极小化问题中函数值下降速度最快的方向**，而**如果目标函数的二阶导数信息可用**，使用二阶导数的方法（即**牛顿类方法**），**收敛速度的精度通常更高**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5fe4b627",
   "metadata": {},
   "source": [
    "理解二阶导数带来的优势并不困难，假设 $f(x),x\\in \\mathbb{R}^p $ 是我们优化的目标函数，我们要求 $f(x)$ 的极小值：\n",
    "$$\n",
    "\\min_{x\\in\\mathbb{R}^p} f(x)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2ad549e",
   "metadata": {},
   "source": [
    "* **梯度下降**法在当前点 $x_k$ 做 $f(x)$ 的**一阶Tayler近似**\n",
    "    $$\n",
    "    f(x) = f(x_k) + \\nabla f(x_k)^T(x - x_k)\n",
    "    $$\n",
    "    以迭代到下一个点 $x_{k+1}$，这**等价于用一次平面来逼近曲面** $f(x)$，优点是计算简单，缺点是**逼近精度有限**\n",
    "* 使用**二阶导数**的**牛顿类方法**当前点 $x_k$ 做 $f(x)$ 的**二阶Tayler近似**\n",
    "    $$\n",
    "    f(x) = f(x_k) + \\nabla f(x_k)^T(x - x_k) + \\frac{1}{2}(x - x_k)^T\\nabla^2 f(x_k) (x - x_k)\n",
    "    $$\n",
    "    以迭代到下一个点 $x_{k+1}$，这**等价于用二次平面来逼近曲面** $f(x)$，优点是**逼近精度高**，缺点是**需要付出包括二阶导数和黑塞矩阵** $\\nabla^2 f(x)\\in\\mathbb{R}^{p\\times p}$ 在内的**额外的计算代价**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39dafd9b",
   "metadata": {},
   "source": [
    "因此，**使用了高阶导数信息的优化算法，收敛速度和求解精度通常更高**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3359aab1",
   "metadata": {},
   "source": [
    "我们具体来看**牛顿类方法的优化思想**，设 $g_k = \\nabla f(x_k), H_k = \\nabla^2 f(x_k)$ 分别表示函数在 $x_k$ 处的梯度和黑塞矩阵，则二阶近似："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "279ebf02",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) = f(x_k) + g_k^T(x - x_k) + \\frac{1}{2}(x - x_k)^T H_k(x - x_k)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fc503e1",
   "metadata": {},
   "source": [
    "假设要极小化 $f(x)$，则上式是关于 $x\\in\\mathbb{R}^p$ 的二次函数，令导数等于0，可以得到极值点为： "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d7bbd8",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(x^*) = g_k + H_k(x^*-x_k) = 0 \\quad\\Rightarrow\\quad x^* = x_k - H_k^{-1}g_k\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddd7f318",
   "metadata": {},
   "source": [
    "因此，**牛顿类方法的下一步迭代公式**为：\n",
    "$$\n",
    "x_{k+1} = x_k - H_k^{-1}g_k, \\quad k=0,1,\\cdots\n",
    "$$\n",
    "其中，$H_k^{-1}g_k$ 被称为**牛顿方向**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa84f56a",
   "metadata": {},
   "source": [
    "**回到XGBoost**，在考虑第 $b$ 个基学习器的求解时，待优化的目标函数为："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b868d87",
   "metadata": {},
   "source": [
    "$$\n",
    "\\min_{\\gamma_b} \\mathcal{L_b} = \\min_{\\gamma_b} \\left( \\sum_{i=1}^{N} L\\left( y_i,f_{b-1}(x_i) +  G_b(x_i;\\gamma_b) \\right) + \\Omega(G_b) \\right)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5feaa499",
   "metadata": {},
   "source": [
    "对目标函数在 $f=f_b{-1}$ 处进行二阶Tayler近似，得到："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13e475ce",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L_b} = \\sum_{i=1}^{N} \\big\\{ L\\left(y_i,f_{b-1}(x_i)\\right) + g_{b,i} G_b(x_i;\\gamma_b) + \\frac{1}{2}h_{b,i}G_b^2(x_i;\\gamma_b) \\big\\} + \\Omega(G_b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "790b8404",
   "metadata": {},
   "source": [
    "其中，$g_{b,i},H_{b,i}$ 分别表示**损失函数在每个样本上的一阶和二阶偏导数**："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a25fe34",
   "metadata": {},
   "source": [
    "$$\n",
    "g_{b,i} = \\frac{\\partial L(y_i,f(x_i))}{\\partial f}\\big|_{f=f_{b-1}},\\quad h_{b,i} = \\frac{\\partial^2 L(y_i,f(x_i))}{\\partial f^2}\\big|_{f=f_{b-1}},\\quad i=1,\\cdots,N\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cf03af5",
   "metadata": {},
   "source": [
    "注意，下面的量在目标函数表达式中，均与 $G_b(x)$ 无关，它们可以视为优化的常数：\n",
    "$$\n",
    "f_{b-1}(x_i),\\quad L(y_i,f_{b-1}(x_i)),\\quad g_{b,i},\\quad h_{b,i},\\quad i=1,\\cdots,N\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "858f7625",
   "metadata": {},
   "source": [
    "**丢弃无关常数**，目标函数可以简写为："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "534237f7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L_b} = \\sum_{i=1}^{N} \\big\\{ g_{b,i} G_b(x_i;\\gamma_b) + \\frac{1}{2}h_{b,i}G_b^2(x_i;\\gamma_b) \\big\\} + \\Omega(G_b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73bac7c4",
   "metadata": {},
   "source": [
    "我们将树 $G_b(x)$ 的结构代入，得到："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac314175",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathcal{L}_b & = & \\displaystyle{ \\sum_{i=1}^{N} \\big\\{ g_{b,i} \\left(\\sum_{j=1}^{J_b}c_{b,j} \\mathbb{I}(x\\in R_{b,j})\\right) + \\frac{1}{2}h_{b,i}\\left(\\sum_{j=1}^{J_b}c_{b,j}^2 \\mathbb{I}(x\\in R_{b,j})\\right)  \\big\\} + \\tau J_b + \\frac{1}{2}\\lambda \\sum_{j=1}^{J_b}c_{b,j}^2 } \\\\\n",
    "& = & \\displaystyle{ \\sum_{j=1}^{J_b}\\left[ \\left(\\sum_{x\\in R_{b,j}}{g_{b,i}} \\right) c_{b,j} + \\frac{1}{2}\\left( \\sum_{x\\in R_{b,j}} h_{b,i} + \\lambda \\right)c_{b,j}^2 \\right] + \\tau J_b } \\\\\n",
    "& = & \\displaystyle{ \\sum_{j=1}^{J_b}\\left[ G_{b,j} c_{b,j} + \\frac{1}{2}\\left( H_{b,j} + \\lambda \\right)c_{b,j}^2 \\right] + \\tau J_b } \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d41bd6d4",
   "metadata": {},
   "source": [
    "其中，$G_{b,j}.H_{b,j}$ 分别各叶子结点上一阶和二阶导数的统计量："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ef44a34",
   "metadata": {},
   "source": [
    "$$\n",
    "G_{b,j} = \\sum_{x\\in R_{b,j}}{g_{b,i}},\\quad H_{b,j} = \\sum_{x\\in R_{b,j}} h_{b,i},\\quad j=1,\\cdots,J_b\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d4d7aa7",
   "metadata": {},
   "source": [
    "$\\mathcal{L_b}$ 对 $c_{b,j}$ 求导，令导数等于0，可以**得到每个叶子结点上的最优预测为**："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69a95895",
   "metadata": {},
   "source": [
    "$$\n",
    "c_{b,j}^* = -\\frac{G_{b,j}}{H_{b,j}+\\lambda}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5c545f7",
   "metadata": {},
   "source": [
    "代入目标函数，可以得到**给定决策树的划分** $\\mathcal{R}=\\{R_{b,j}\\}_{j=1}^{J_b}$ 时，**最优目标函数值**为："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99f59ddd",
   "metadata": {},
   "source": [
    "$$\n",
    "\\tilde{\\mathcal{L}}_b(\\mathcal{R}) = -\\frac{1}{2}\\sum_{j=1}^{J_b}\\frac{G_{b,j}^2}{H_{b,j}+\\lambda} + \\tau J_b\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57645b3d",
   "metadata": {},
   "source": [
    "$\\tilde{\\mathcal{L}}_b(\\mathcal{R})$ 被称为结构 $\\mathcal{R}$ 的**结构分数**，用于**综合衡量模型拟合效果和模型复杂度，它的值越小越好**，和式中的每一项即为每个结点 $j$ 处的结构分数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac51f3c5",
   "metadata": {},
   "source": [
    "**基于结构分数，我们就可以制定决策树的分裂规则**了，分裂的核心思想不变，我们需要扫描**所有可分裂特征** $k$，**及该特征的切分点** $s$，寻找**最优的分裂对** $(k,s)$，具体的分裂算法叙述如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88170b03",
   "metadata": {},
   "source": [
    "**(Algo) XGBoost决策树分裂算法**：\n",
    "* (1) 给定决策树的某个结点 $n$，计算**该结点处的结构分数**：\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}(n) = -\\frac{1}{2} \\frac{ \\left( \\sum_{x_i\\in n} g_{b,i}\\right)^2 }{ \\sum_{x_i\\in n} h_{b,i} + \\lambda } + \\tau\n",
    "$$\n",
    "* (2) 依次选择切分特征 $k$，及该特征的切分点 $s$，构成切分对 $(k,s)$\n",
    "    * (2.1) 按照切分对 $(k,s)$ **将该结点处的样本划分到左右两个子结点** $n_L,n_R$：\n",
    "        * 如果特征 $k$ 是数值型特征：\n",
    "        $$\n",
    "        n_L = \\{x\\in n|x^{(k)} <= s\\},\\quad n_R = \\{x\\in n|x^{(k)} > s\\}\n",
    "        $$\n",
    "        * 如果特征 $k$ 是类别型特征：\n",
    "        $$\n",
    "        n_L = \\{x\\in n|x^{(k)} = s\\},\\quad n_R = \\{x\\in n|x^{(k)} \\ne s\\}\n",
    "        $$\n",
    "    * (2.2) 切分后，分别**计算左右两个子结点处的结构分数**及子结构的结构分数和：\n",
    "    $$\n",
    "    \\tilde{\\mathcal{L}}(n_L) = -\\frac{1}{2} \\frac{ \\left( \\sum_{x_i\\in n_L} g_{b,i}\\right)^2 }{ \\sum_{x_i\\in n_L} h_{b,i} + \\lambda } + \\tau \\\\\n",
    "    \\tilde{\\mathcal{L}}(n_R) = -\\frac{1}{2} \\frac{ \\left( \\sum_{x_i\\in n_R} g_{b,i}\\right)^2 }{ \\sum_{x_i\\in n_R} h_{b,i} + \\lambda } + \\tau \\\\\n",
    "    \\tilde{\\mathcal{L}}(n_L) + \\tilde{\\mathcal{L}}(n_R) = -\\frac{1}{2} \\frac{ \\left( \\sum_{x_i\\in n_L} g_{b,i}\\right)^2 }{ \\sum_{x_i\\in n_L} h_{b,i} + \\lambda } -\\frac{1}{2} \\frac{ \\left( \\sum_{x_i\\in n_R} g_{b,i}\\right)^2 }{ \\sum_{x_i\\in n_R} h_{b,i} + \\lambda } + 2\\tau\n",
    "    $$\n",
    "    * (2.3) 计算该分裂 $(k,s)$ 带来的增益 $\\text{Gain}(k,s)$：\n",
    "        $$\n",
    "        \\text{Gain}(k,s) = \\tilde{\\mathcal{L}}(n) - [\\tilde{\\mathcal{L}}(n_L) + \\tilde{\\mathcal{L}}(n_R)]\n",
    "        $$\n",
    "        注意，$\\text{Gain}(k,s)$ **可能是负值**，这表明**分裂只会带来增益的下降**，此时不再进行分裂，实际中，会使用某个阈值参数 $\\varepsilon > 0$，只有当 $\\text{Gain}(k,s) > \\varepsilon$ 时，才考虑这个分裂 $(k,s)$\n",
    "    * (2.4) 如果分裂 $(k,s)$ **能够带来可信的增益，就保存这次分裂信息**\n",
    "* (3) 决定最终的分裂方案\n",
    "    * (3.1) 如果**可信增益分裂的集合不为空**，从中选择**分裂增益最大的组合** $(k^*,s^*)$ 作为最优分裂决策，然后分裂该结点\n",
    "    * (3.2) 如果**可信增益分裂的集合为空集**，意味着**没有进一步分裂该结点的意义**，停止分裂"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23e7b5d2",
   "metadata": {},
   "source": [
    "上述的分裂决策规则，在考虑子树结构复杂度的同时，也体现了**预剪枝**思想"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ff121a9",
   "metadata": {},
   "source": [
    "#### **1.2.3 近似分裂算法**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d28854c0",
   "metadata": {},
   "source": [
    "结构分数导出的增益确定了XGBoost算法的分裂规则，但**在真正的算法执行中，会遇到计算复杂度过高的问题**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7a9fb48",
   "metadata": {},
   "source": [
    "对大多数的梯度提升算法，上面介绍的贪心的结点分裂算法是**精确贪心算法**（**Exact Greedy**），这种算法在考虑切分对 $(k,s)$ 时，会顺序选择每一个切分特征 $k$，和特征 $x^{(k)}$ 的每一个可能的切分值 $s$，我们将精确贪心算法描述如下："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa6fa214",
   "metadata": {},
   "source": [
    "**(Algo) 结点分裂的精确贪心算法（Exact Greedy）**：\n",
    "* **输入**：当掐结点 $n$，当前轮次 $b$ 和前 $b-1$ 步得到的模型 $f_{b-1}$，特征维度 $p$，样本量为 $N$，惩罚权重 $\\lambda,\\tau>0$\n",
    "* **输出**：分裂后的结点 $n_L,n_R$\n",
    "* **算法流程**：\n",
    "    * (1) 初始化：\n",
    "    $$ \n",
    "    \\text{Gain} = 0,\\quad \\text{G} = \\sum_{x_i\\in n} g_{b,i}, \\quad \\text{H} = \\sum_{x_i\\in n} h_{b,i}\n",
    "    $$\n",
    "    * (2) **依次选择** $k=1,2,\\cdots,p$：\n",
    "        * (2.1) 初始化：$G_L = 0,H_L = 0$\n",
    "        * (2.2) 将结点 $n$ 上的数据**根据特征** $x^{(k)}$ **进行排序**\n",
    "        * (2.3) **依次选择** $j=1,2,\\cdots,N$：\n",
    "            * (2.3.1) 计算 $G_L, G_R, H_L, H_R$：\n",
    "            $$\n",
    "            G_L = G_L + g_{b,j},\\quad H_L = H_L + h_{b,j}\\\\\n",
    "            G_R = G - G_L,\\quad H_R = H_R - H_L\\\\\n",
    "            $$\n",
    "            上面的计算，等价于考虑了切分对 $(k,x_j^{(k)})$\n",
    "            * (2.3.2) 计算增益：\n",
    "            $$\n",
    "            \\text{Gain} = \\max\\big\\{ \\text{Gain}, \\frac{1}{2} \\left[\\frac{G_L^2}{H_L+\\lambda} + \\frac{G_R^2}{H_R+\\lambda} - \\frac{G^2}{H + \\lambda}\\right] - \\tau  \\big\\}\n",
    "            $$\n",
    "    * (3) 让结点 $n$ 在最大 $\\text{Gain}$ 对应的切分对 $(k,s)$ 上分裂"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5afeaacd",
   "metadata": {},
   "source": [
    "上述算法会带来极高的计算复杂度：\n",
    "* 一方面，假设特征数量为 $p$，样本量为 $N$，则完成一轮扫描所需要的复杂度为 $\\mathcal{O}(pN)$（**依次选择每一个特征，和该特征上每一个可能的取值**），这在 $p$ 和 $N$ 较大的时候都不可行\n",
    "* 另一方面，在考虑某个切分对 $(k,s)$ 时，我们还**需要将样本根据特征** $x^{(k)}$ **进行排序**，然后根据切分点 $s$ 比较大小，将样本划分到左右结点，**排序操作同样需要花费大量时间**\n",
    "* **排序时我们通常需要将样本导入内存**，这在 $p,N$ 很大时给硬件带来了很大压力，容易**出现内存紧张的问题**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "133f85a8",
   "metadata": {},
   "source": [
    "因此，我们希望减少结点分裂的计算量，提出一种**近似算法**，在**降低运算成本的同时，不要损耗过多的性能**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "475df203",
   "metadata": {},
   "source": [
    "基本的想法是，对每个变量 $k$，**仅挑选少数几个切分点** $S_k=\\{s_{k,1},\\cdots,s_{k,l}\\}$ 进行扫描：\n",
    "$$\n",
    "s_{k,1} \\leq s_{k,2}\\leq \\cdots \\leq s_{k,l}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83a7fdf4",
   "metadata": {},
   "source": [
    "自然地，第一个问题是，如何确定切分点集合 $S_k$？一种直观的想法是**抽取分位数**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea8fb802",
   "metadata": {},
   "source": [
    "**抽取分位数本质上是对特征** $x^{(k)}$ 的分布进行等分，从而利用分位点，但XGBoost在此基础上更进一步，它使用了**加权分位点**，加权分位点的思想相当于对特征 $x^{(k)}$ 的取值进行了**重要性排序**，根据排序结果，再**选出值得尝试的切分点**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1111c5b5",
   "metadata": {},
   "source": [
    "回顾我们对于第 $b$ 个基学习器的目标函数："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ade53",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L_b} = \\sum_{i=1}^{N} \\big\\{ g_{b,i} G_b(x_i;\\gamma_b) + \\frac{1}{2}h_{b,i}G_b^2(x_i;\\gamma_b) \\big\\} + \\Omega(G_b)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0cb52aad",
   "metadata": {},
   "source": [
    "注意 $\\{h_{b,i}\\}_{i=1}^{N}$ 是与优化无关的常数，可以将上式变形："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb052b3a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathcal{L}_b & = & \\displaystyle{ \\sum_{i=1}^{N} \\big\\{ g_{b,i} G_b(x_i;\\gamma_b) + \\frac{1}{2}h_{b,i}G_b^2(x_i;\\gamma_b) \\big\\} + \\Omega(G_b) }\\\\\n",
    "& = & \\displaystyle{ \\sum_{i=1}^{N} \\frac{1}{2}h_{b,i}\\big\\{ 2\\frac{g_{b,i}}{h_{b,i}} G_b(x_i;\\gamma_b) + G_b^2(x_i;\\gamma_b) \\big\\} + \\Omega(G_b) }\\\\\n",
    "& = & \\displaystyle{ \\sum_{i=1}^{N} \\frac{1}{2}h_{b,i}\\big\\{\\left(\\frac{g_{b,i}}{h_{b,i}} \\right)^2 + 2\\frac{g_{b,i}}{h_{b,i}} G_b(x_i;\\gamma_b) + G_b^2(x_i;\\gamma_b) - \\left(\\frac{g_{b,i}}{h_{b,i}} \\right)^2 \\big\\} + \\Omega(G_b) }\\\\\n",
    "& = & \\displaystyle{ \\frac{1}{2}\\sum_{i=1}^{N} h_{b,i}\\left( G_b(x_i;\\gamma_b) - \\left[-\\frac{g_{b,i}}{h_{b,i}}\\right]\\right)^2 + \\Omega(G_b) + C }\\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddeca602",
   "metadata": {},
   "source": [
    "配方后，可以看到目标函数是一个**以** $\\{h_{b,i}\\}_{i=1}^{N}$ **为权重的平方损失**，**基学习器** $G_b(x;\\gamma_b)$ **拟合的目标是牛顿方向**：\n",
    "$$\n",
    "\\big\\{ -\\frac{g_{b,i}}{h_{b,i}} \\big\\}_{i=1}^{N}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1b2f138",
   "metadata": {},
   "source": [
    "因此，样本 $(x_i,y_i)$ 对于目标函数的贡献度等于其二阶导数 $h_{b,i}$，我们可以**根据** $\\{h_{b,i}\\}_{i=1}^{N}$ **来完成特征的重要性排序**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05b976f9",
   "metadata": {},
   "source": [
    "**XGBoost给出了如下的特征重要性排名函数** $r_k(z) : \\mathbb{R}\\to [0,1]$，假设**损失函数是凸函数**，则 $h_{b,i} > 0$，对于某个节点 $n$，某个特征 $k$，任意 $z\\in\\mathbb{R}$："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "750b7126",
   "metadata": {},
   "source": [
    "$$\n",
    "r_k(z) = \\frac{1}{\\sum_{x_i \\in n} h_{b,i}} \\sum_{x_i\\in n, x_i^{(k)} < z} h_{b,i}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ac3556e",
   "metadata": {},
   "source": [
    "可以看到 $r_k(z)$ 的分子是所有第 $k$ 个特征 $x^{(k)} < z$ 的样本对应的 $h_{b,i}$ 的和"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3f7ae31",
   "metadata": {},
   "source": [
    "之后，选择某个**近似因子** $\\varepsilon > 0$，寻找**尽可能等间隔的切分点** $S_k=\\{s_{k,1}\\cdots,s_{k,l}\\}$ 使得：\n",
    "$$\n",
    "|r_k(s_{k,j}) - r_k(s_{k,j+1})| < \\varepsilon, \\forall j\\quad s_{k,1} = \\min_i x_i^{(k)},s_{k,l} = \\max_i x_i^{(k)}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f57e49a4",
   "metadata": {},
   "source": [
    "简单来说，即**两个切分点的间隔要保证小于** $\\varepsilon$，同时**两个切分点包含的样本数量要尽可能多**，根据以上准则，**大约要挑选出** $1/\\varepsilon$ **个切分点**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c338cec7",
   "metadata": {},
   "source": [
    "因此，$\\varepsilon$ 控制了切分点挑选的**近似程度**，所以称为**近似因子**，$\\varepsilon$ **越小，近似程度越高，挑选的切分点越多，算法计算复杂度也就越高**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e29f43b2",
   "metadata": {},
   "source": [
    "下图以9个样本的特征取值 $x_i^{(k)}$、二阶导数 $h_{b,i}$ 及其重要性排名结果 $r_k(z)$ 为示例："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b68af12",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/加权分位数.png\" width=800>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d29b6534",
   "metadata": {},
   "source": [
    "可以看到，二阶导数的总和等于 $\\sum_{i} h_{b,i} = 2.0$，**如果取** $\\varepsilon=1/4$，**则9个样本将会被划分为如上的4组**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dbf68da",
   "metadata": {},
   "source": [
    "**基于加权分位数**，下面我们来叙述XGBoost在处理决策树分裂时的**近似分裂算法**，算法本质和**精确贪心算法**没有区别，只是切分点的选择上使用了加权分位数法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "588990a2",
   "metadata": {},
   "source": [
    "**(Algo) 结点分裂的近似分裂算法（Approximate Split）**：\n",
    "* **输入**：当掐结点 $n$，当前轮次 $b$ 和前 $b-1$ 步得到的模型 $f_{b-1}$，特征维度 $p$，样本量为 $N$，近似因子 $\\varepsilon$，惩罚权重 $\\lambda,\\tau>0$\n",
    "* **输出**：分裂后的结点 $n_L,n_R$\n",
    "* **算法流程**：\n",
    "    * (1) **依次选择每个特征** $k=1,2,\\cdots,p$：\n",
    "        * (1.1) 利用二阶导数和函数 $r_k(z)$，为特征 $k$ 准备好**切分点** $S_k=\\{s_{k,1},\\cdots,s_{k,l}\\}$，这有两种方案可选：\n",
    "            * **全局方案（global）**：在每棵树，即每个基学习器训练前计算出 $S_k$\n",
    "            * **局部方案（local）**：在树的每个结点，即每次分裂前计算出 $S_k$\n",
    "    * (2) **依次选择每个特征** $k=1,2,\\cdots,p$：\n",
    "        * (2.1) **依次选择切分点集** $S_k$ **中的每个点** $v=2,3,\\cdots,l$，计算：\n",
    "        $$\n",
    "        G_{k,v} = \\sum_{x_i\\in n, s_{k,v-1}\\leq x_i^{(k)} < s_{k,v}} g_{b,i},\\quad H_{k,v} = \\sum_{x_i\\in n, s_{k,v-1}\\leq x_i^{(k)} < s_{k,v}} h_{b,i}\n",
    "        $$\n",
    "    * (3) 类似**精确贪心算法**中的步骤，寻找到最大增益 $\\text{Gain}$，从而确定最优分裂"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78a8a77f",
   "metadata": {},
   "source": [
    "针对两种 $S_k$ 的计算方法，我们做简单的补充说明：\n",
    "* **全局方案（global）**：我们在**一棵树的生成之前**，就已经计算好每个特征的分裂点 $S_k$，并且**在整个树的生成过程当中，用的都是一开始计算的分裂点**，这也就代表了，使用**全局策略的开销更低**，但**如果分裂点不够多的话，会牺牲部分性能**\n",
    "* **局部方案（local）**：在**分裂时，对每一个结点所包含的样本，重新计算其所有特征的分裂点** $S_k$，在一棵树分裂的时候，**样本会逐渐被划分到不同的结点中**，因此**每个结点所包含的样本，以及这些样本有的特征取值是不一样的**，因此对每个结点重新计算分裂点**可以更好保证模型性能**，相当于是**因地制宜**。但显然**局部方法的计算开销更大**，使用**局部方法分裂点数目不用太多**，即近似因子 $\\varepsilon$ 可以取稍大一些，也能取得很好的效果"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65b76210",
   "metadata": {},
   "source": [
    "下图展示了**精确贪心算法**和两种方案下**近似分裂算法**XGBoost模型的性能表现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2382e0d",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/近似分裂算法对比.png\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63e24f22",
   "metadata": {},
   "source": [
    "横轴表示基学习器个数，纵轴是测试集AUC，从图中我们可以看到：\n",
    "* **近似分裂算法能取得和精确贪心算法**几乎一样好的性能，证明了近似分裂算法的有效性\n",
    "* 我们还注意到，**全局方案的近似分裂算法需要取较小的近似因子**，如果近似因子太大，切分点太**粗糙**，模型的性能会大打折扣\n",
    "* **局部方案的近似分裂算法取较大的近似因子也能获得非常好的效果**，图中 $\\varepsilon=0.3$ 意味着每次分裂只需要遍历3个切分点，这极大减少了计算量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8b6d257",
   "metadata": {},
   "source": [
    "#### **1.2.4 处理缺失数据或熊稀疏数据**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a641c5c3",
   "metadata": {},
   "source": [
    "**如果数据中存在缺失值，则它们没办法加入到算法的计算过程中**，通常的做法是：\n",
    "* 从数据中**删除它们**，根据情况不同，可以考虑删除缺失值所在的行（即**删除样本**），或删除缺失值所在的列（即**删除特征**）\n",
    "* 基于某种假设用其他值**填充缺失值**，例如用**没有缺失样本的均值、众数**等填充"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33ec8b15",
   "metadata": {},
   "source": [
    "另一种容易给传统机器学习算法带来困难的问题是**稀疏矩阵**问题，即**特征矩阵** $X\\in\\mathbb{R}^{N\\times p}$ **包含大量的零元素**，这在对原始特征进行**独热编码（One-Hot）**时尤为常见，如下面的例子所示："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d21bf70f",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/独热编码.png\" width=900>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd2afdb",
   "metadata": {},
   "source": [
    "**独热编码带来特征维度的快速膨胀，同时特征取值包含大量的零，也可以视为一种缺失值**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ad04568",
   "metadata": {},
   "source": [
    "**XGBoost针对这种缺失/稀疏数据改动了分裂算法，使XGBoost能够直接处理缺失/稀疏数据**，而无需我们手动处理，我们仅介绍大致思想"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6ad83a2",
   "metadata": {},
   "source": [
    "* 依然假设我们考虑要在第 $k$ 个特征上分裂\n",
    "* 选择 $x_i^{(k)}$ 不缺失的样本 $\\{x_i\\in n | x_i^{(k)} \\text{不缺失}\\}$，用它们来确定切分点集合 $S_k$\n",
    "* 在切分点 $s_{k,v}$ 处，将未缺失的样本按照 $x_i^{(k)} \\leq s_{k,v}$ 和 $x_i^{(k)} > s_{k,v}$ 划分后，再**考虑两种情况**：\n",
    "    * (1) **将所有缺失样本划分到左子结点**，或者\n",
    "    * (2) **将所有缺失样本划分到右子结点**\n",
    "* **比较 (1) 和 (2) 哪种划分带来的增益** $\\text{Gain}$ 更大，保存相应的分裂方案"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09df1a14",
   "metadata": {},
   "source": [
    "下图展示了考虑了特征稀疏性后算法在运算速度上的提升"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4b02899",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/稀疏性算法比较.png\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "28f4f249",
   "metadata": {},
   "source": [
    "**相比原始算法，考虑缺失/稀疏特性的新的分裂算法速度能够加快超过50倍**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c64b2e",
   "metadata": {},
   "source": [
    "#### **1.2.5 其他特性**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42a2fef8",
   "metadata": {},
   "source": [
    "剩下的一些特性主要包括：\n",
    "* XGBoost也支持**行抽样和列抽样**，即**随机森林**中的样本采样和特征采样\n",
    "* XGBoost也引入了**Shrinkage方案**，通过一个 $\\eta < 1$ 的学习率参数，减缓单个基学习器的学习进度，减少过拟合\n",
    "* 与传统Boosting方法一样，**可以设置每个基学习器的拟合能力**，如控制树深\n",
    "* 支持**分块并行运算**，注意在结点的分裂过程中，分裂对 $(k,s)$ 的划分、选择和计算对每个特征是独立的，因此XGBoost在**特征层面实现并行**，以加速算法\n",
    "    * **请注意这和随机森林的区别**，**随机森林的并行学习是多个基学习器可以并行**，XGBoost依然是Boosting类方法，**基学习器的训练无法并行，只能串行**，这里并行加速是在每个基学习器的训练过程中，结点分裂的特征搜索时可以并行处理"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "80467625",
   "metadata": {},
   "source": [
    "### **1.3 XGBoost模型总结**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22763e4e",
   "metadata": {},
   "source": [
    "最后，我们简单总结一下XGBoost模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d2a5aa0",
   "metadata": {},
   "source": [
    "* 通过**使用二阶导数的信息参与优化**，XGBoost的**模型性能进一步提升**\n",
    "* XGBoost现在**支持任何损失函数作为优化目标**，用户在自定义损失函数的同时，只需要**提供一阶和二阶导数**即可\n",
    "* XGBoost**在基学习器建模时就考虑了正则化问题**，在损失函数中加入了两个正则化项（叶结点个数和叶结点输出的L2惩罚），这种**预剪枝**策略让**用户在使用XGBoost时无需太多担心过拟合问题**\n",
    "* XGBoost**通过近似分裂算法来加速结点的分裂过程**，同时保证几乎不损失模型性能\n",
    "* XGBoost**增加了对缺失/稀疏数据的处理机制**，在面对这种“病态”数据时，有更强的处理能力\n",
    "* XGBoost**使用了Shrinkage策略**，降低每个基学习器的贡献度，**进一步降低模型方差，减缓过拟合**\n",
    "* XGBoost**使用了随机森林中的样本抽样和特征抽样策略**，减少每个基学习器之间的相关性，**降低模型方差，提升泛化能力**\n",
    "* XGBoost**通过特征分块并行来加速结点分裂**，结点在考虑分裂对时可以借助并行同时快速处理多个特征"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "081eefc5",
   "metadata": {},
   "source": [
    "更多XGBoost模型技术的介绍，可以在`/source/Chap9`目录下找到原论文"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b600010",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d27cefc3",
   "metadata": {},
   "source": [
    "## **2 LightGBM**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db71d0e8",
   "metadata": {},
   "source": [
    "XGBoost已经在传统的Boosting框架进行了大量的改进和优化，使Boosting算法更上一层楼，但**在工业界面对海量的数据和特征，XGBoost依然存在缺点**："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "57904c2e",
   "metadata": {},
   "source": [
    "* 无论是使用**精确贪心算法**或者**近似分裂算法**，**建造决策树或分裂前总需要对数据进行预排序**\n",
    "    * 排序所花费的时间，**增加计算成本**\n",
    "    * 保存排序的索引以便使用时快速查找需要开辟额外的内存，我们需要一张和 $X$ 相同规模的表来存储每个特征的排序情况，需要原数据两倍的内存量，**增加硬件成本**，当数据量较大时（百万样本），XBGoost非常容易爆内存，以至于无法训练\n",
    "* 即使使用近似分裂算法加速，**遍历切分点的复杂度依然是** $\\mathcal{O}(\\# data)$，时间复杂度过大，训练缓慢"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f03c828",
   "metadata": {},
   "source": [
    "于是，微软开源了另一种梯度提升改进框架**LightGBM**（**Light Gradient Boosting Machine**），它使用树模型作为基学习器，同样可作为**分布式算法**供工业界使用，并提供以下应用优势："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ec01f80",
   "metadata": {},
   "source": [
    "* 更快的训练速度和更高的训练效率\n",
    "* 更低的内存消耗\n",
    "* 取得更好的模型性能\n",
    "* 提供并行、分布式和GPU训练\n",
    "* 可以处理海量数据（TB级数据）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "639e4ebc",
   "metadata": {},
   "source": [
    "### **2.1 LightGBM的改进**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "afe4d18e",
   "metadata": {},
   "source": [
    "#### **2.1.1 直方图算法：Histogram-based Algorithm**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f333baa9",
   "metadata": {},
   "source": [
    "LightGBM使用直方图算法，**将连续型特征放入离散化的箱子（bin）中**，然后用这些箱子构建**特征直方图**，模型直接基于特征直方图寻找最佳的分裂点"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "041e9adb",
   "metadata": {},
   "source": [
    "如下图所示，我们展示将某个特征的10个样本，**经过直方图算法后，将其编码为直方图特征**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ee06434",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/直方图算法.png\" width=900>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c18ca9e4",
   "metadata": {},
   "source": [
    "直方图算法需要注意：\n",
    "* 算法**将原始数值特征的浮点数**，通过直方图**将每个样本放入到对应的箱子中**，并**用箱子编号替换特征取值**\n",
    "* 直方图算法可以将数据数据类型从`float64`（占用**8比特**）数据转换为`int8`（占用**1字节**）来进行存储，大大减少存储数据所需要的内存空间（**所需的内存大小仅需要原来的** $1/8$）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "631d4eb5",
   "metadata": {},
   "source": [
    "构建直方图的时间复杂度大约为 $\\mathcal{O}(N\\times p)$，**假设直方图箱子的个数为** $k$，**则寻找最佳分裂对** $(k,s)$ **的复杂度仅为** $\\mathcal{O}(kp)$，模型训练的速度因此而提高，而且因为**不需要像XGBoost那样存储排序索引，也无需额外的内存开销了**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6941b7d9",
   "metadata": {},
   "source": [
    "使用直方图算法的另一个加速技巧是**直方图做差加速**，通常构建某个结点的直方图时，我们**需要遍历该叶子上的所有数据来完成统计**，直方图做差指出：**一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到**，此时**直方图做差只需要遍历直方图的** $k$ **个箱**，利用该技巧，**LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图**，在**速度上可以提升一倍**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41c2929f",
   "metadata": {},
   "source": [
    "直方图做差的示意图如下所示："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c392f3dd",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/直方图做差.png\" width=900>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eb3705f",
   "metadata": {},
   "source": [
    "使用**直方图算法的提出可能会引发如下思考**：离散化数值特征后，对特征的切分不再精细，这肯定会对模型性能有影响，我们不需要担心这种性能损耗吗？\n",
    "* 一方面，这种操作带来的影响不大，因为**梯度提升的过程中这种损耗会被越来越多的基学习器弥补**\n",
    "* 另一方面，这种操作有时甚至效果更好，我们将数值特征转换为少数的几个整数，对特征做了**模糊化处理**，这能起到**正则化效果**，防止模型过拟合"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8e3a0a24",
   "metadata": {},
   "source": [
    "最后一提，由于直方图算法实现简单，能带来的训练速度的提升明显，**现在版本的XGBoost也支持了直方图算法**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8cd09268",
   "metadata": {},
   "source": [
    "#### **2.1.2 单边梯度采样GOSS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a4cb3dc",
   "metadata": {},
   "source": [
    "**在AdaBoost中，样本权重指示了数据样本在基学习器训练时的重要性**，而在梯度提升的统一框架中，我们没有样本权重的概念"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e162d15",
   "metadata": {},
   "source": [
    "但在梯度提升算法中，**梯度是个非常有效的信息，它的大小能够帮助我们对样本采样**，如果某个样本 $(x_i,y_i)$ 有一个**很小的梯度值**，那么说明**该样本已经接近损失函数** $L(y_i,f(x_i))$ **的极小值点**，该**样本的训练误差很小，模型在该样本上的表现很好**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "838b8d77",
   "metadata": {},
   "source": [
    "但我们能否在训练时**直接剔除这部分小梯度样本，让它们不参与训练**，类比准备期末考试，我们只做难题，不做简单题可行吗？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c494333e",
   "metadata": {},
   "source": [
    "**答案是不可以**，因为如果**完全提出小梯度样本，数据的分布会发生改变**，模型的拟合会出现**有偏估计问题**，直接损害模型性能"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f5fc02e3",
   "metadata": {},
   "source": [
    "LightGBM提出了**单边梯度采样**（**Gradient-based One-Side Sampling**，简称**GOSS**）来解决这个问题，简单来说，它**保留所有大梯度样本，然后从小梯度样本中随机采样**，这样能**缓解数据分布改变的问题**，同时**减少了训练样本，加快了训练速度**，类比考前刷题，相当于难题都要做，简单题随机挑着做"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f9f725c",
   "metadata": {},
   "source": [
    "GOSS的操作思路大致如下：\n",
    "* (1) 计算每个样本的**梯度的绝对值**，假设样本量为 $N$，确认**大梯度比例阈值** $\\alpha\\in(0,1)$ 和**采样比例阈值** $\\beta\\in(0,1)$\n",
    "* (2) 将前 $\\alpha \\cdot N$ 大的梯度样本全部保留，得到样本子集 $A$\n",
    "* (3) 对剩下的 $(1-\\alpha)\\cdot N%$ 的小梯度样本们，随机抽样出 $\\beta \\cdot N%$ 的样本，得到样本子集 $B$\n",
    "* (4) 在计算增益时，**对于小梯度样本集** $B$ 得到的结果，**乘以因子以调整权重**，这一步的目的是**修正数据分布**：\n",
    "$$\n",
    "\\text{Gain} = \\text{Gain}(A) + \\frac{1 - \\alpha}{\\beta} \\text{Gain}(B)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7f9b56f",
   "metadata": {},
   "source": [
    "#### **2.1.3 互斥特征捆绑EFB**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7feede8",
   "metadata": {},
   "source": [
    "XGBoost在处理高维缺失/稀疏特征时会尝试将这些病态数据统一划分到左子结点或右子结点，**LightGBM从稀疏数据的特点出发，提出了互斥特征捆绑**（**Exclusive Feature Bundling。简称EFB**）技术"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bd0794f",
   "metadata": {},
   "source": [
    "**对于稀疏特征，许多特征取值为0元素**，因此很多特征是“**相互排斥**”的，这里的相互排斥的意思是，**同一个样本在这些特征上，经常很多特征同时为0，或者说从不同时取非0值**，EFB基于这种想法，**将互斥的特征捆绑成一捆**（**Bundle**），组成一个新的特征，从而实现**特征压缩，降低维度**的操作"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "317aeae4",
   "metadata": {},
   "source": [
    "EFB的含义如下图所示，前面3列稀疏特征构成一组**互斥特征**，算法会将他们捆绑到一起处理，后面两列是稠密特征，**稠密特征不用参与EFB过程**，简单理解，可以**将EFB视为One-Hot的逆过程**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "047411ff",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/EFB示意图.png\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4ece6c1b",
   "metadata": {},
   "source": [
    "接下来需要解决的有两个：\n",
    "* **应该将哪些特征捆绑在一起？**\n",
    "* **如何捆绑这些特征？**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "066a79c9",
   "metadata": {},
   "source": [
    "**(1) 将哪些特征捆绑在一起？**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53cef204",
   "metadata": {},
   "source": [
    "简单来说，我们希望将互斥的特征捆绑在一起，**LightGBM将互斥特征集的构造类比为图着色问题**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6507ef5",
   "metadata": {},
   "source": [
    "给定一个无向图 $G=(V,E)$，其中 $V$ 表示顶点集合，$E$ 表示边集合，**图着色问题将把** $V$ **分为** $k$ **个不同的颜色组，每个组形成一个独立集合，使集合中没有任何相邻的顶点**，最终目的是**最小化所需要的颜色数量** $k$，如下图所示："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d971f96e",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/图着色问题.png\" width=200>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82c16258",
   "metadata": {},
   "source": [
    "上面由5个顶点构成的图，**至少需要 4 种颜色才能完成着色**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f06232a3",
   "metadata": {},
   "source": [
    "因此，**EFB从特征出发构造图**，将**每个特征** $V_i=x^{(i)}$ **作为顶点**，并在**两两特征之间构造边** $E_{i,j}$，**边的权重** $w_{i,j}$ **设置为冲突值**，**冲突值定义为两个特征上不同时取0的样本数**：\n",
    "$$\n",
    "w_{i,j} = \\#\\{x^{(i)}\\text{和}x^{(j)}\\text{不同时取0} \\}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2f35b1c1",
   "metadata": {},
   "source": [
    "冲突值，或者说**权重** $w_{i,j}$ **越大**，从图上来讲说明两个**特征之间的连接关系越紧密**，我们越**不能用同一种颜色给这两个特征上色**，即**它们不能被捆绑到一起**\n",
    "* 考虑特殊情况会发现，**稠密特征不能和任何其他特征捆绑到一起**，因为和稠密特征相连接的边的权重 $w_{i,j}$ 永远取最大值 $N$（即样本量）\n",
    "* 虽然**这种构图方式任何特征之间都有连接关系**，但我们可以**假设那些权重** $w_{i,j}$ **较小的边没有实际的连接关系**，此时**EFB就可以将连接关系较弱的特征合并为一个组，用同一个颜色着色**，即完成了**捆绑**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f956bc7",
   "metadata": {},
   "source": [
    "**图着色问题是个NP-Hard问题**，无法在多项式时间内获得最优解决方案，**EFB采用贪心算法完成特征分组**，大致想法是：\n",
    "* 首先对特征进行计算，完成构图 $G=(V,E,W)$，其中 $W$ 表示边是包含权重的\n",
    "* **基于图中特征顶点的度，对特征进行降序排列**\n",
    "* 顺序遍历每个特征 $k=1,2,\\cdots,p$：\n",
    "    * 将特征 $k$ 放置在某个已经构造的Bundle里\n",
    "    * 如果**放置后整个Bundle的冲突值小于阈值** $\\gamma$，则进行放置\n",
    "    * 如果**放置后整个Bundle的冲突值超过阈值** $\\gamma$，则**创建一个仅包含特征** $k$ **的新Bundle**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6de422cd",
   "metadata": {},
   "source": [
    "最后，我们给出**EFB的贪心捆绑算法**如下"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97ee62ee",
   "metadata": {},
   "source": [
    "**(Algo) EFB的贪心捆绑**\n",
    "* **输入**：特征集合 $K=\\{1,2,\\cdots,p\\}$，冲突阈值 $\\gamma$\n",
    "* **输出**：捆绑后的特征集：Bundles $B$\n",
    "* **算法流程**：\n",
    "    * 构造图 $G=(V,E,W)$，初始化Bundels $B=\\{\\}$，Bundles冲突值 $C=\\{\\}$\n",
    "    * **根据特征顶点的度对特征进行降序排列**\n",
    "    * **依次选择**每个特征 $k=1,2,\\cdots,p$：\n",
    "        * 设定`newBundle = True`（即**默认创建新Bundle**）\n",
    "        * 从Bundles $B$ 中**依次选择**每个Bundle $b$\n",
    "            * 计算特征 $k$ 和Bundle $b$ 的冲突值 $c$\n",
    "            * 取出Bundle $b$ 的冲突值 $bc = BC[b]$，判断：\n",
    "                * 如果 $c + bc < \\gamma$，则将特征 $k$ 加入Bundle $b$\n",
    "                * 设置`newBundle = False`，即不需要创建新的Bundle来保存特征 $k$\n",
    "                * **跳出该层循环**，迭代下一个特征\n",
    "        * 判断`newBundle`是否为`True`：\n",
    "            * 如果`newBundle = True`，则创建一个新的Bundle，并将特征 $k$ 放入该Bundle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffc3adeb",
   "metadata": {},
   "source": [
    "最后的输出Bundles $B$ 应该是**集合的集合**，每个**集合元素就表示被捆绑在一起的特征**，所有**稠密特征应该作为单元素集出现**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8db9ce9",
   "metadata": {},
   "source": [
    "**(2) 如何将特征捆绑在一起？**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d668911",
   "metadata": {},
   "source": [
    "EFB在捆绑特征时，**基本想法是将两个特征的取值叠加**，但这样操作会带来问题"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2d0632e",
   "metadata": {},
   "source": [
    "假设特征 $f_A$ 的取值范围是 $(0,10)$，特征 $f_B$ 的取值范围是 $(0,20)$，如果简单进行 $f_A + f_B$，则对于一个合并后的特征值 $r\\in(0,10)$，**模型将无法区别它来自** $f_A$ **还是** $f_B$，这就**给模型的学习带来影响，降低模型性能**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45f4f3ef",
   "metadata": {},
   "source": [
    "因此一种正确的做法是，**在合并特征的时候，添加一个偏移量**，这个偏移量等于 $f_A$ 的最大值：\n",
    "* 如果合并前特征来自 $f_A$，则合并后特征也取值 $f_A$\n",
    "* 如果合并前特征来自 $f_B$，则合并后特征取值 $f_B + \\max\\{f_A\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a444acb2",
   "metadata": {},
   "source": [
    "这样就不会造成混淆了，如果合并后特征值小于 $\\max\\{f_A\\}$，则它一定来自特征 $f_A$，如果取值大于 $\\max\\{f_B\\}$，则一定来自特征 $f_B$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb4d1fe",
   "metadata": {},
   "source": [
    "但如上操作可能会出现疑问：如果合并前 $f_A,f_B$ 同时都有值怎么办？这不也会造成混淆吗？\n",
    "* 确实，LightGBM将这种情况定义为**冲突**，并且**LightGBM允许少量这种冲突的存在**\n",
    "* **被捆绑在同一个Bundle中的特征，出现同时不为0的情况很少**，因为**大量同时不0的情况在特征捆绑着色时，EFB就不会把它们捆绑到一起**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21b49763",
   "metadata": {},
   "source": [
    "**特征捆绑的操作是在直方图算法的结果上进行的**，如果我们用直方图来描述特征捆包操作，会非常清晰，如下图所示"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "22a77a07",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/特征捆绑合并直方图.png\" width=900>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db3e76d4",
   "metadata": {},
   "source": [
    "#### **2.1.4 Leaf-wise生长策略**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "866185fa",
   "metadata": {},
   "source": [
    "一般的梯度提升方法，包括XGBoost在内，**树的分裂过程是按层分裂（Level-wise）**的，而**LightGBM的分裂是按页分裂（Leaf-wise）**，如果你学习过算法和数据结构，那么这可以类比为**广度优先**和**深度优先**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19c59361",
   "metadata": {},
   "source": [
    "XGBoost每次分裂会**依次对相同深度的所有结点进行**，这样会**同时产生很多分裂增益比较低的子结点**，虽然**宽大的树不容易过拟合**，但是增加了很多不必要的搜索和分裂"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c6d4891",
   "metadata": {},
   "source": [
    "LightGBM为了**节省计算开销进一步加速算法**，提出了Leaf-wise分裂策略，具体地，**树的生长策略从分裂增益最大的叶子节点不断向下生长**，这样做有两个好处：\n",
    "* **减少不必要的搜索，节省系统开销**\n",
    "* **有效提高模型拟合精度，增强模型性能**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35c6d43f",
   "metadata": {},
   "source": [
    "但**Leaf-wise策略会使得模型容易过拟合**，因此，**LightGBM一般会限制决策树的最大深度，避免过度拟合**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f07f28f8",
   "metadata": {},
   "source": [
    "两种生长策略的对比图如下所示"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a89bf3bc",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/Level-wise和Leaf-wise.png\" width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60cf267f",
   "metadata": {},
   "source": [
    "总之，**LightGBM使用Leaf-wise生长策略是为了节省了系统开销加速算法**，且**保证模型精度不被损失太多**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97568e0b",
   "metadata": {},
   "source": [
    "#### **2.1.5 many-vs-many处理类别变量**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a03b359a",
   "metadata": {},
   "source": [
    "一般的机器学习模型，包括XGBoost在内比较头疼的问题就是处理**类别变量**（**Categorical Features**），通常的做法是在数据预处理阶段，我们需要手动将类别变量进行某种编码：\n",
    "* 例如**独热编码将其扩展为稀疏的0-1矩阵**，稀疏特征对很多机器学习算法都不友好，并且**容易使决策树对特征空间的划分变得零碎**，容易过拟合，影响性能\n",
    "* 或者**顺序编码**，将类别变量的每种取值按照 $1,2,\\cdots$ 编码，这种编码**人为引入了特征取值间的相对大小关系**，很多时候不太合理，例如类别变量时城市，那么人为给城市之间赋予大小关系很难解释"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a00caf7b",
   "metadata": {},
   "source": [
    "LightGBM使用**many-vs-many的策略**，**使算法能够直接处理类别变量，而无需我们提前使用某种编码方法**，在介绍many-vs-many策略之前，我们先说明之前模型所使用的**one-vs-rest策略**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42181660",
   "metadata": {},
   "source": [
    "设特征 $k$ 是类别特征，可能的特征取值包括 $A_k=\\{a_{k,1},\\cdots,a_{k,m}\\}$\n",
    "* **one-vs-rest策略**：在分裂时，**选择** $A_k$ **中的某个元素** $a_{k,i}$，然后根据：\n",
    "    $$\n",
    "    x_i^{(k)} = a_{k,i},\\quad x_i^{(k)}\\ne a_{k,i}\n",
    "    $$\n",
    "    将样本划分到左右子结点，然后计算增益，进行比较\n",
    "* **many-vs-many策略**：在分裂时，**选择** $A_k$ **中的某个子集** $\\mathcal{A} = \\{a_{k,i_1},\\cdots,a_{k,i_s}\\}$，然后根据：\n",
    "    $$\n",
    "    x_i^{(k)} \\in \\mathcal{A},\\quad x_i^{(k)}\\notin \\mathcal{A}\n",
    "    $$\n",
    "    将样本划分到左右子结点，然后计算增益，进行比较"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9867462",
   "metadata": {},
   "source": [
    "假设某个特征包含的可能取值集合为`[北京，上海，广州]`，下图直观的表示了两种策略的区别"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eceb3578",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/两种策略可视化.png\" width=700>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a394c96b",
   "metadata": {},
   "source": [
    "**many-vs-many分裂方式，可以基于类别数据直接进行分裂，方法留给模型的学习空间更大，不易过拟合**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23bd5923",
   "metadata": {},
   "source": [
    "下面需要考虑的问题是，**如何选择类别变量的切分点，使特征集合划分为many-vs-many的两个子集**，**穷尽所有可能可以把特征值集合** $A_k=\\{a_{k,1},\\cdots,a_{k,m}\\}$ **划分为两个非空子集的所有方案是不现实的**，这需要付出太多的计算量了，所需枚举的次数为：\n",
    "$$\n",
    "C_m^1 + C_m^2 + \\cdots + C_m^{\\lfloor m/2\\rfloor} \\sim \\mathcal{O}(2^m)\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84ffe897",
   "metadata": {},
   "source": [
    "LightGBM的做法是，设特征 $k$ 是类别特征，可能的特征取值包括 $A_k=\\{a_{k,1},\\cdots,a_{k,m}\\}$，计算每种取值下的排序变量 $y_{k,j}$：\n",
    "$$\n",
    "y_{k,j} = \\frac{G}{H} = \\frac{ \\sum_{x_i^{(k)} = a_{k,j}} g_i }{ \\sum_{x_i^{(k)} = a_{k,j}} h_i },\\quad j=1,2,\\cdots,m\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5e1cc80",
   "metadata": {},
   "source": [
    "其中，$g_i$ 表示样本 $x_i$ 的一阶导数，$h_i$ 表示样本 $x_i$ 的二阶导数，然后**类别特征值** $A_k$ **按照** $\\{y_{k,j}\\}_{j=1}^{m}$ **的大小进行排序**，然后**将排序后的每个特征取值依次作为切分点**，将特征划分为many-vs-many的两个子集，进入决策树的分裂过程，如下图所示："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "184c2fd6",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/类别变量切分.png\" width=1100>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e6c5897c",
   "metadata": {},
   "source": [
    "这样，所需要考虑的分裂方案数仅有 $m-1$ 种，大大减少了计算量，$G/H$ 在本章的讨论中已经是我们的老朋友了，**它是基学习器使用牛顿法拟合的方向，以它作为排序依据很自然，也很有意义**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "913a9428",
   "metadata": {},
   "source": [
    "### **2.2 LightGBM总结**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c520ae2a",
   "metadata": {},
   "source": [
    "我们最后来总结一下LightGBM的改进和优势"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "546a16c3",
   "metadata": {},
   "source": [
    "* 使用**直方图算法**，将数值特征直接打散到少数几个箱中，**极大加快算法速度，减少存储特征所需的内存**，减少切分点的遍历，**现在XGBoost也支持了直方图算法**\n",
    "* 使用**单边梯度采样GOSS**，过滤小梯度样本的同时，通过加权修正数据分布，**加速算法，也几乎不影响模型性能**\n",
    "* 使用**互斥特征捆绑EFB**来处理**稀疏/缺失**特征，能够**降低特征维度，进一步减少内存开销，加速算法**\n",
    "* 使用**Leaf-wise策略进行决策树的生长**，**减少不必要的搜索和分裂，减少系统开销**\n",
    "* **支持直接考虑类别变量，而无需提前对类别变量编码**，使用**many-vs-many策略**，让模型的决策流行更统一，进一步提升性能"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a2845c4",
   "metadata": {},
   "source": [
    "此外，**LightGBM也支持XGBoost的一些其他优点，例如Shrinkage，样本采样和特征采样等**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd5ebefe",
   "metadata": {},
   "source": [
    "在工程方面，**LightGBM还做了一些其他的工程优化**，这些优化与机器学习算法设计无关，主要是计算机工程实现，包括：\n",
    "* **并行优化（包括特征并行、数据并行和投票并行）**，减少并行处理时的通信开销，加速算法\n",
    "* **增加cache命中率**，XGBoost的缓存访问是随机的，因此cache命中率低，影响机器性能，LightGBM在实现时对缓存的访问是顺序访问\n",
    "* LightGBM**支持GPU训练**，现在XGBoost也支持GPU训练了"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "332e7c4a",
   "metadata": {},
   "source": [
    "总的来说，LightGBM整体上更多是工程实现上对梯度提升算法的优化，在较大数据集上，**LightGBM的训练速度相比XGBoost可以快几十倍，而二者的性能几乎相同**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63e8a14e",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f515afd4",
   "metadata": {},
   "source": [
    "## **3 Stacking**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f76df947",
   "metadata": {},
   "source": [
    "最后，我们介绍一种简单的集成方法，**它不同于Bagging和Boosting**，往往是机器学习比赛中的常用集成技巧，**堆叠模型**（**Stacking**）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a62da56",
   "metadata": {},
   "source": [
    "### **3.1 Stacking方法流程**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9334b66",
   "metadata": {},
   "source": [
    "我们在第一章就介绍过机器学习建模中一个很重要的问题：**没有免费的午餐**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "987fdb62",
   "metadata": {},
   "source": [
    "这意味着，**不同的模型对数据有着不同假设**，无论是从**模型性能**出发考虑，还是有其他需求，**一个模型不可能在所有的数据和问题上都表现最好**，即使**对于同一个数据集，不同模型在不同样本上的表现也有不同**，有的模型可能更**擅长掌握数据的总体特征**，有的模型可能**对于极端值更稳健**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2fd681e1",
   "metadata": {},
   "source": [
    "因此我们希望提出一种方法，能够**同时采纳不同模型的优点，将它们集成在一起**，而无需我们做出模型选择（成年人的世界：我全都要😍😍😍）"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f698a367",
   "metadata": {},
   "source": [
    "Stacking方法**通过多层堆叠的方式来训练机器学习模型**，从而实现对不同种类模型的集成，Stacking中包含两个学习器概念：**基学习器**（**base model**）和**元学习器**（**meta model**），为了解释Stacking的流程，我们直接看算法流程"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10ba0753",
   "metadata": {},
   "source": [
    "**(Algp) Stacking**\n",
    "* **输入1**：训练数据集 $D=\\{(x_i,y_i)\\}_{i=1}^{N_1}$ 和测试数据集 $T=\\{(x_i,y_i)\\}_{i=1}^{N_2}$，折树 $K$\n",
    "* **输入2**：$m$ 个**基学习器模型** $\\{B_1,B_2,\\cdots,B_m\\}$，基学习器模型可以完全不同，以泛化不同基学习器的拟合特长，例如：\n",
    "    * $m = 3$ 时，如果是**回归问题**，考虑使用`决策树`、`SVR`和`线性回归`，如果是**分类问题**，考虑使用`决策树`、`SVC`和`逻辑回归`\n",
    "    * $m = 3$ 时，考虑基学习器都使用决策树，但不同的决策树设置不同的最大树深度，或者设置不同的损失函数，即**在超参数上让基学习器不同**\n",
    "* **输入3**：提供 1 个**元学习器模型** $M$，该模型最好要**能够处理特征之间相关性比较强的数据**，需要选择对于输入相关性不敏感的模型，例如`决策树`或者`Ridge`\n",
    "* **输出**：**Stacking**模型结构\n",
    "* **算法流程**：\n",
    "    * (1) 将**训练数据集** $D$ 按照**交叉验证**的方式，打散为 $K$ 折，得到 $\\{D^{(k)}\\}_{k=1}^{K}$\n",
    "    * (2) **依次选择**每一折数据 $D^{(k)}, k=1,2,\\cdots,K$\n",
    "        * (2.1) 构造该轮的**训练集** $D_1 = D - D^{(k)}$ 和**验证集** $D_2 = D^{(k)}$\n",
    "        * (2.2) **依次选择**每个基学习器 $B_j,j=1,2,\\cdots,m$\n",
    "            * (2.2.1) 让 $B_j$ 在 $D_1$ 上训练，并在 $D_2$ 上做出预测，得到预测结果 $\\boldsymbol{v}_j^{(k)}$，维度 $N_1/K$\n",
    "            * (2.2.1) 让 $B_j$ 在测试机 $T$ 上做出预测，得到预测结果 $\\boldsymbol{t}_{j}^{(k)}$，维度 $N_2$\n",
    "        * (2.3) $m$ 个基学习器训练完成后，会得到 $m$ 组基学习器在 $D^{(k)}$ 上的预测 $\\boldsymbol{v}_j^{(k)}, j=1,\\cdots,m$，将它们**按列拼接**，构成一个矩阵：\n",
    "            $$\n",
    "            \\boldsymbol{v}^{(k)} = \\left[\\boldsymbol{v}_1^{(k)},\\cdots, \\boldsymbol{v}_m^{(k)}\\right] \\in\\mathbb{R}^{N_1/K \\times m}\n",
    "            $$\n",
    "            注意预测 $\\boldsymbol{v}_j^{(k)}$ 是**包外预测**，即预测时使用的模型 $B_j$ 是没有见过验证数据 $D^{(k)}$ 的\n",
    "        * (2.4) 同样地，我们会得到 $m$ 组基学习器在 $T$ 上的预测 $\\boldsymbol{t}_j^{(k)}, j=1,\\cdots,m$，将它们按列拼接，构成一个矩阵：\n",
    "            $$\n",
    "            \\boldsymbol{t}^{(k)} = \\left[\\boldsymbol{t}_1^{(k)},\\cdots, \\boldsymbol{t}_m^{(k)}\\right] \\in\\mathbb{R}^{N_2 \\times m}\n",
    "            $$\n",
    "    * (3) $K$ 轮完成后，会得到 $K$ 组**验证集**上的预测 $\\boldsymbol{v}^{(k)},k=1,\\cdots,K$，将它们**按行拼接**，构成一个矩阵：\n",
    "        $$\n",
    "        \\boldsymbol{v} = \\left[\\begin{array}{c}\n",
    "        \\boldsymbol{v}^{(1)} \\\\\n",
    "        \\boldsymbol{v}^{(2)} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\boldsymbol{v}^{(K)}\n",
    "        \\end{array}\\right] \\in\\mathbb{R}^{N_1 \\times m}\n",
    "        $$\n",
    "    * (4) $K$ 轮完成后，会得到 $K$ 组**测试集**上的预测 $\\boldsymbol{t}^{(k)},k=1,\\cdots,K$，将它们**求均值**，构成一个矩阵：\n",
    "        $$\n",
    "        \\boldsymbol{t} = \\frac{1}{K}\\sum_{k=1}^{K} \\boldsymbol{t}^{(k)} \\in\\mathbb{R}^{N_2\\times m}\n",
    "        $$\n",
    "    * (5) 最后，让**元学习器** $M$ **在特征** $\\boldsymbol{v}\\in\\mathbb{R}^{N_1\\times m}$ **上训练**，测试集预测时使用特征 $\\boldsymbol{v}\\in\\mathbb{R}^{N_2\\times m}$\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "435d10d3",
   "metadata": {},
   "source": [
    "**Stacking算法的示意图**如下所示："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0541dec7",
   "metadata": {},
   "source": [
    "<img src=\"./source/Chap9/stacking.png\" width=1200>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "47fb8914",
   "metadata": {},
   "source": [
    "我们对算法中的一些细节进行补充说明：\n",
    "* 本质上，Stacking就是**将第一层基学习器的包外预测值**（**验证集上的预测值**）作为**第二层元学习器的输入特征**\n",
    "* 分别对**回归**和**分类**任务，基学习器的预测稍微调整：\n",
    "    * 如果是**回归任务**，基学习器直接预测标签 $y$\n",
    "    * 如果是**分类任务**，基学习器**预测输出概率值**，而不直接预测类别，将概率作为第二层模型的输入特征\n",
    "* 第二层**元学习器的输入特征** $v$ **的维度等于基学习器的个数** $m$\n",
    "* 由于第二层模型本质上是**用标签来预测标签**，并且多个**基学习器预测共同的标签一定会导致特征矩阵** $v$ **的特征有很强的相关性**，所以元学习器最好选择对相关性不敏感的模型，例如决策树、带L2正则化的回归模型等 \n",
    "* 注意到，基学习器的构建使用了交叉验证，交叉验证的目的是为了防止**标签泄露**，如果**不使用交叉验证，基学习器的预测和训练使用同一个集合，这非常容易导致元学习器模型产生过拟合**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c84c2050",
   "metadata": {},
   "source": [
    "为了更清晰地理解Stacking模型，我们直接动手实现，**假设我们处理的是二分类问题**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3add8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyStackingClassifier(X_train, y_train, X_test, base_models : dict, meta_model, K : int=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : np.ndarray\n",
    "        训练集特征和标签\n",
    "    X_test : np.ndarray\n",
    "        测试机特征\n",
    "    base_models : dict\n",
    "        基学习器字典，key表示模型名称，value是模型实例\n",
    "    meta_model : Any\n",
    "        元学习器\n",
    "    K : int, default = 5\n",
    "        交叉验证折数\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    y_pred_train, y_pred_test : np.ndarray\n",
    "        元学习器在训练集和测试集上的预测\n",
    "    \"\"\"\n",
    "    # 划分交叉验证数据集\n",
    "    spliter = KFold(n_splits=K)\n",
    "    m = len(base_models) # 基学习器个数\n",
    "    N1, N2 = len(X_train), len(X_test) # 训练集和测试机样本量\n",
    "\n",
    "    # 初始化元学习器特征\n",
    "    meta_train = np.zeros((N1,m)) # 即矩阵 v\n",
    "    meta_test = np.zeros((N2,m)) # 即矩阵 t\n",
    "\n",
    "    # 第一层模型\n",
    "    # 依次遍历每一折\n",
    "    for k,(train_idx,valid_idx) in enumerate(spliter.split(X_train,y_train)):\n",
    "        print(f\"正在处理第 {k+1} 折...\")\n",
    "        # 获取训练集和验证集\n",
    "        train_data, train_label = X_train[train_idx], y_train[train_idx]\n",
    "        valid_data, valid_label = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "        # 依次遍历每个基学习器\n",
    "        for j,model in enumerate(base_models.values()):\n",
    "            estimator = deepcopy(model)\n",
    "            # 拟合模型\n",
    "            estimator.fit(train_data,train_label)\n",
    "\n",
    "            # 在验证集做出概率预测，构建 Stacking 特征\n",
    "            meta_train[valid_idx,j] = estimator.predict_proba(valid_data)[:,1]\n",
    "            # 在测试集做出预测，构建 Stacking 特征\n",
    "            meta_test[:,j] += estimator.predict_proba(X_test)[:,1] / K # 除以折数，取均值\n",
    "    \n",
    "    # 查看每个基学习器在训练集上的性能\n",
    "    for j,name in enumerate(base_models.keys()):\n",
    "        print(\"基学习器 %-8s\\t 在测试集上的AUC：%.4f\"%(name,roc_auc_score(y_train,meta_train[:,j])))\n",
    "    \n",
    "    # 第二层模型，拟合元学习器\n",
    "    print(\"正在拟合元学习器...\")\n",
    "    meta_model.fit(meta_train,y_train)\n",
    "    y_pred_test = meta_model.predict_proba(meta_test)\n",
    "    \n",
    "    return meta_test, y_pred_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b2d1522",
   "metadata": {},
   "source": [
    "### **3.2 Stacking方法说明**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7b39826",
   "metadata": {},
   "source": [
    "首先关于Stacking方法的适用场景，使用Stacking方法有两个理由：\n",
    "* 用来**榨干**模型最后的性能\n",
    "* 各个基学习器模型之间的差异较大，想要得到综合利用"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0c5924f",
   "metadata": {},
   "source": [
    "Stacking方法**不适合用于需要模型解释性**的建模问题，两层堆叠的模型使我们很难掌握特征重要性，模型结构也较为复杂"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d4ebfa7",
   "metadata": {},
   "source": [
    "对比Bagging方法，Stacking的不同点主要在于：\n",
    "* Bagging使用很**多个相同的基学习器**来拟合，而Stacking使用**少数几个完全不同的基学习器**\n",
    "* Bagging最终的预测**仅是对基学习器预测的投票或平均聚合**，Stacking还需要一个元学习器拟合，**用标签来预测标签**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f08d12a8",
   "metadata": {},
   "source": [
    "此外，**基学习器的选择应该尽可能差异化**，**因为类似的模型容易犯类似的错误**，而Stacking就是希望综合各种模型的优点，使用相同的基学习器意义不大"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f3f85f7",
   "metadata": {},
   "source": [
    "最后，**Stacking方法的另一个好朋友Blending也具有类似的思想**，实现方法更加简单，**Blending取消了交叉验证**，而是用**hold-out**策略，即仅留一折"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a592eb5e",
   "metadata": {},
   "source": [
    "直接看Blending的代码会更加清晰，可以对比Stacking比较二者的异同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abcb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyBlendingClassifier(X_train, y_train, X_test, base_models : dict, meta_model, valid_size : float=0.3):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : np.ndarray\n",
    "        训练集特征和标签\n",
    "    X_test : np.ndarray\n",
    "        测试机特征\n",
    "    base_models : dict\n",
    "        基学习器字典，key表示模型名称，value表示模型实例\n",
    "    meta_model : Any\n",
    "        元学习器\n",
    "    valid_size : float, default = 0.3\n",
    "        hold-out留出验证集的比例\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    y_pred_test : np.ndarray\n",
    "        元学习器在测试集上的预测\n",
    "    \"\"\"\n",
    "    m = len(base_models) # 基学习器个数\n",
    "\n",
    "\n",
    "    # hold-out 留一折\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train,y_train,test_size=valid_size)\n",
    "\n",
    "    N1, N2 = len(X_valid), len(X_test) # 训练集和测试机样本量\n",
    "    # 初始化元学习器特征\n",
    "    meta_train = np.zeros((N1,m)) # 即矩阵 v\n",
    "    meta_test = np.zeros((N2,m)) # 即矩阵 t\n",
    "\n",
    "    # 依次遍历每个基学习器\n",
    "    for j,model in enumerate(base_models.values()):\n",
    "        estimator = deepcopy(model)\n",
    "        # 拟合模型\n",
    "        estimator.fit(X_train,y_train)\n",
    "\n",
    "        # 在验证集做出概率预测，构建 Blending 特征\n",
    "        meta_train[:,j] = estimator.predict_proba(X_valid)[:,1]\n",
    "        # 在测试集做出概率预测，构建 Blending 特征\n",
    "        meta_test[:,j] += estimator.predict_proba(X_test)[:,1]\n",
    "\n",
    "    # 查看每个基学习器在训练集上的性能\n",
    "    for j,name in enumerate(base_models.keys()):\n",
    "        print(\"基学习器 %-8s\\t 在测试集上的AUC：%.4f\"%(name,roc_auc_score(y_train,meta_train[:,j])))\n",
    "    \n",
    "    # 第二层模型，拟合元学习器\n",
    "    print(\"正在拟合元学习器...\")\n",
    "    meta_model.fit(meta_train,y_valid)\n",
    "    y_pred_test = meta_model.predict_proba(meta_test)\n",
    "    \n",
    "    return meta_test, y_pred_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e0ce041",
   "metadata": {},
   "source": [
    "### **3.3 Stacking实验**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c01015e",
   "metadata": {},
   "source": [
    "我们生成一个简单的二分类数据，样本量`10000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41efa36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "X, y = make_blobs(n_samples = 10000,centers = 2,random_state = 0,cluster_std = 1.5)\n",
    "# 划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.30,random_state = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2e45e3a",
   "metadata": {},
   "source": [
    "样本量比较多，我们**抽取一个子集来进行简单的可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9d0e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd6f83b6ee0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_sample, y_sample = resample(*(X_train, y_train),replace=False,n_samples=200)\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "plt.scatter(X_sample[y_sample==0,0],X_sample[y_sample==0,1],color=colors[0],label=\"Negative\")\n",
    "plt.scatter(X_sample[y_sample==1,0],X_sample[y_sample==1,1],color=colors[1],label=\"Positive\")\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "plt.title(\"Scatter plot of subset samples\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "721fea7b",
   "metadata": {},
   "source": [
    "接下来，创建一组基学习器，**包含下面的 3 个模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae89ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用字典存储模型\n",
    "base_models = {\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5), # KNN\n",
    "    \"支持向量机\": SVC(C=1.0,kernel=\"rbf\",probability=True), # 支持向量机\n",
    "    \"随机森林\": RandomForestClassifier(n_estimators=10,n_jobs=-1,criterion=\"gini\"), # 随机森林\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe70cc87",
   "metadata": {},
   "source": [
    "再使用GBDT作为元学习器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87358f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义元学习器\n",
    "meta_model = GradientBoostingClassifier(n_estimators=30,learning_rate=0.02,subsample=0.5,max_depth=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e31fda2",
   "metadata": {},
   "source": [
    "使用Stacking来训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f23e3ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理第 1 折...\n",
      "正在处理第 2 折...\n",
      "正在处理第 3 折...\n",
      "正在处理第 4 折...\n",
      "正在处理第 5 折...\n",
      "基学习器 KNN     \t 在测试集上的AUC：0.9327\n",
      "基学习器 支持向量机   \t 在测试集上的AUC：0.9362\n",
      "基学习器 随机森林    \t 在测试集上的AUC：0.9337\n",
      "正在拟合元学习器...\n"
     ]
    }
   ],
   "source": [
    "test_feature, y_test_pred = MyStackingClassifier(X_train,y_train,X_test,base_models,meta_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cdf9b923",
   "metadata": {},
   "source": [
    "最后来比较**基学习器**和**Stacking模型**在测试集上的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a05003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基学习器 KNN     \t 在测试集上的AUC：0.9251\n",
      "基学习器 支持向量机   \t 在测试集上的AUC：0.9212\n",
      "基学习器 随机森林    \t 在测试集上的AUC：0.9290\n",
      "Stacking模型在测试集上的AUC：0.9369\n"
     ]
    }
   ],
   "source": [
    "for j,name in enumerate(base_models.keys()):\n",
    "    print(\"基学习器 %-8s\\t 在测试集上的AUC：%.4f\"%(name,roc_auc_score(y_test,test_feature[:,j])))\n",
    "print(\"Stacking模型在测试集上的AUC：%.4f\"%(roc_auc_score(y_test,y_test_pred[:,1])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15959a66",
   "metadata": {},
   "source": [
    "可以看到Stacking模型性能有少幅度提升，但提升不多，这就是Stacking所谓的“**榨干模型的最后一丝性能**”"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f981203",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc2d7559",
   "metadata": {},
   "source": [
    "## **练习**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3c51a02c",
   "metadata": {},
   "source": [
    "1. 比较和总结Bagging。Boosting，XGBoost和LightGBM四个模型，加深自己的理解"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e235a9b",
   "metadata": {},
   "source": [
    "2. 学习和使用XGBoost和LightGBM，这两个算法包在**使用时调整超参数是关键**，根据官方提示**总结它们的关键参数有哪些**，应该**如何调整这些参数以获得优秀的模型**？\n",
    "    * XGBoost官方文档：\n",
    "        * 中文文档：https://xgboost.apachecn.org/#/README\n",
    "        * 官方文档：https://xgboost.readthedocs.io/en/latest/\n",
    "    * LightGBM官方文档：\n",
    "        * 中文文档：https://lightgbm.cn/\n",
    "        * 官方文档：https://lightgbm.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8346a01",
   "metadata": {},
   "source": [
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
